{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5512551,"sourceType":"datasetVersion","datasetId":3179746}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:40.072844Z","iopub.execute_input":"2024-03-27T09:34:40.073715Z","iopub.status.idle":"2024-03-27T09:34:40.083783Z","shell.execute_reply.started":"2024-03-27T09:34:40.073671Z","shell.execute_reply":"2024-03-27T09:34:40.082461Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"/kaggle/input/twitter-propaganda-classification/twitter_dataset_translated_ukrainian.csv\n/kaggle/input/twitter-propaganda-classification/twitter_dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport seaborn as sns\nimport pandas as pd\n\nfrom IPython.display import display, Markdown","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:40.087604Z","iopub.execute_input":"2024-03-27T09:34:40.089169Z","iopub.status.idle":"2024-03-27T09:34:40.099864Z","shell.execute_reply.started":"2024-03-27T09:34:40.089111Z","shell.execute_reply":"2024-03-27T09:34:40.098226Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"display(Markdown(\"**Full Dataset:**\"))\ndata = pd.read_csv('/kaggle/input/twitter-propaganda-classification/twitter_dataset.csv')\ndisplay(data.head())\ndisplay(\"Full Dataset Shape: \", data.shape)\n\ndisplay(Markdown(\"**Clean (Non-Propagandistic) Text Dataset:**\"))\nclean_data = data[data['is_propaganda'] == False]\nclean_data = clean_data[['text']]\ndisplay(clean_data.head())\ndisplay(\"Clean Text Dataset Shape: \",clean_data.shape)\n\ndisplay(Markdown(\"**Human Propaganda Dataset:**\"))\nhuman_propaganda = data[data['is_propaganda'] == True]\nhuman_propaganda = human_propaganda[['text']]\ndisplay(human_propaganda.head())\ndisplay(\"Human Propaganda Text Dataset Shape: \",human_propaganda.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:40.116321Z","iopub.execute_input":"2024-03-27T09:34:40.116779Z","iopub.status.idle":"2024-03-27T09:34:40.604769Z","shell.execute_reply.started":"2024-03-27T09:34:40.116746Z","shell.execute_reply":"2024-03-27T09:34:40.603531Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Full Dataset:**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   Unnamed: 0                   id                 created_at  \\\n0        1749  1514553915580329988  2022-04-14 10:39:27+00:00   \n1        2409  1510803460320632839  2022-04-04 02:16:28+00:00   \n2        2463  1475560113536741379  2021-12-27 20:12:00+00:00   \n3         116  1527722359314075649  2022-05-20 18:46:08+00:00   \n4        2742  1517110124325879808  2022-04-21 11:56:54+00:00   \n\n                                                text  is_propaganda  \n0  Woman who held up poster of Marine Le Pen and ...          False  \n1  ‚ö°Ô∏èZelensky: Around 150,000 people trapped in M...          False  \n2  RT @natomission_ru: üá∑üá∫#Russia Deputy FM Sergey...           True  \n3  #Azovstal fully liberated ‚Äì Russian military\\n...           True  \n4  RT @BloombergUK: \"He was almost foaming at the...          False  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id</th>\n      <th>created_at</th>\n      <th>text</th>\n      <th>is_propaganda</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1749</td>\n      <td>1514553915580329988</td>\n      <td>2022-04-14 10:39:27+00:00</td>\n      <td>Woman who held up poster of Marine Le Pen and ...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2409</td>\n      <td>1510803460320632839</td>\n      <td>2022-04-04 02:16:28+00:00</td>\n      <td>‚ö°Ô∏èZelensky: Around 150,000 people trapped in M...</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2463</td>\n      <td>1475560113536741379</td>\n      <td>2021-12-27 20:12:00+00:00</td>\n      <td>RT @natomission_ru: üá∑üá∫#Russia Deputy FM Sergey...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>116</td>\n      <td>1527722359314075649</td>\n      <td>2022-05-20 18:46:08+00:00</td>\n      <td>#Azovstal fully liberated ‚Äì Russian military\\n...</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2742</td>\n      <td>1517110124325879808</td>\n      <td>2022-04-21 11:56:54+00:00</td>\n      <td>RT @BloombergUK: \"He was almost foaming at the...</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Full Dataset Shape: '"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(12990, 5)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Clean (Non-Propagandistic) Text Dataset:**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                                                 text\n0   Woman who held up poster of Marine Le Pen and ...\n1   ‚ö°Ô∏èZelensky: Around 150,000 people trapped in M...\n4   RT @BloombergUK: \"He was almost foaming at the...\n8   Key UN climate change finding widely misinterp...\n10  Lawyers for the two European tourists argued m...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Woman who held up poster of Marine Le Pen and ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>‚ö°Ô∏èZelensky: Around 150,000 people trapped in M...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>RT @BloombergUK: \"He was almost foaming at the...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Key UN climate change finding widely misinterp...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Lawyers for the two European tourists argued m...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Clean Text Dataset Shape: '"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(6495, 1)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Human Propaganda Dataset:**"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                                                text\n2  RT @natomission_ru: üá∑üá∫#Russia Deputy FM Sergey...\n3  #Azovstal fully liberated ‚Äì Russian military\\n...\n5  'Intense battle' | Russian army surrounds last...\n6  Russia‚Äôs FSB has released footage reportedly s...\n7  Hundreds of activists gathered in Washington, ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>RT @natomission_ru: üá∑üá∫#Russia Deputy FM Sergey...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>#Azovstal fully liberated ‚Äì Russian military\\n...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>'Intense battle' | Russian army surrounds last...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Russia‚Äôs FSB has released footage reportedly s...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Hundreds of activists gathered in Washington, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Human Propaganda Text Dataset Shape: '"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(6495, 1)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Text Generation with a Miniature GPT","metadata":{}},{"cell_type":"code","source":"import os\n\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n\nimport keras\nfrom keras import layers\nfrom keras import ops\nfrom keras.layers import TextVectorization\nimport numpy as np\nimport os\nimport string\nimport random\nimport tensorflow\nimport tensorflow.data as tf_data\nimport tensorflow.strings as tf_strings","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:40.608614Z","iopub.execute_input":"2024-03-27T09:34:40.609022Z","iopub.status.idle":"2024-03-27T09:34:40.620504Z","shell.execute_reply.started":"2024-03-27T09:34:40.608990Z","shell.execute_reply":"2024-03-27T09:34:40.618669Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n    \"\"\"\n    Mask the upper half of the dot product matrix in self attention.\n    This prevents flow of information from future tokens to current token.\n    1's in the lower triangle, counting from the lower right corner.\n    \"\"\"\n    i = ops.arange(n_dest)[:, None]\n    j = ops.arange(n_src)\n    m = i >= j - n_src + n_dest\n    mask = ops.cast(m, dtype)\n    mask = ops.reshape(mask, [1, n_dest, n_src])\n    mult = ops.concatenate(\n        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n    )\n    return ops.tile(mask, mult)\n\n\nclass TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super().__init__()\n        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n        self.ffn = keras.Sequential(\n            [\n                layers.Dense(ff_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs):\n        input_shape = ops.shape(inputs)\n        batch_size = input_shape[0]\n        seq_len = input_shape[1]\n        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n        attention_output = self.dropout1(attention_output)\n        out1 = self.layernorm1(inputs + attention_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output)\n        return self.layernorm2(out1 + ffn_output)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:40.622844Z","iopub.execute_input":"2024-03-27T09:34:40.623271Z","iopub.status.idle":"2024-03-27T09:34:40.641046Z","shell.execute_reply.started":"2024-03-27T09:34:40.623238Z","shell.execute_reply":"2024-03-27T09:34:40.639245Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super().__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = ops.shape(x)[-1]\n        positions = ops.arange(0, maxlen, 1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:40.643036Z","iopub.execute_input":"2024-03-27T09:34:40.643630Z","iopub.status.idle":"2024-03-27T09:34:40.658905Z","shell.execute_reply.started":"2024-03-27T09:34:40.643580Z","shell.execute_reply":"2024-03-27T09:34:40.657551Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def create_model():\n    inputs = layers.Input(shape=(maxlen,), dtype=\"int32\")\n    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n    x = embedding_layer(inputs)\n    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n    x = transformer_block(x)\n    outputs = layers.Dense(vocab_size)(x)\n    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n    model.compile(\n        \"adam\",\n        loss=[loss_fn, None],\n    )  # No loss and optimization based on word embeddings from transformer block\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:40.662771Z","iopub.execute_input":"2024-03-27T09:34:40.663241Z","iopub.status.idle":"2024-03-27T09:34:40.679739Z","shell.execute_reply.started":"2024-03-27T09:34:40.663206Z","shell.execute_reply":"2024-03-27T09:34:40.678616Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def custom_standardization(input_string):\n    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n    lowercased = tf.strings.lower(input_string)\n    stripped_html = tf.strings.regex_replace(lowercased, \"<br />\", \" \")\n    return tf.strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:40.680988Z","iopub.execute_input":"2024-03-27T09:34:40.681394Z","iopub.status.idle":"2024-03-27T09:34:40.696653Z","shell.execute_reply.started":"2024-03-27T09:34:40.681332Z","shell.execute_reply":"2024-03-27T09:34:40.695023Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Define a function to prepare inputs and labels for language modeling\ndef prepare_lm_inputs_labels(text):\n    text = tf.expand_dims(text, -1)\n    tokenized_sentences = vectorize_layer(text)\n    x = tokenized_sentences[:, :-1]\n    y = tokenized_sentences[:, 1:]\n    return x, y","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:40.699680Z","iopub.execute_input":"2024-03-27T09:34:40.700433Z","iopub.status.idle":"2024-03-27T09:34:40.716904Z","shell.execute_reply.started":"2024-03-27T09:34:40.700378Z","shell.execute_reply":"2024-03-27T09:34:40.714797Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class TextGenerator(keras.callbacks.Callback):\n    \"\"\"A callback to generate text from a trained model.\n    1. Feed some starting prompt to the model\n    2. Predict probabilities for the next token\n    3. Sample the next token and add it to the next input\n\n    Arguments:\n        max_tokens: Integer, the number of tokens to be generated after prompt.\n        start_tokens: List of integers, the token indices for the starting prompt.\n        index_to_word: List of strings, obtained from the TextVectorization layer.\n        top_k: Integer, sample from the `top_k` token predictions.\n        print_every: Integer, print after this many epochs.\n    \"\"\"\n\n    def __init__(\n        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n    ):\n        self.max_tokens = max_tokens\n        self.start_tokens = start_tokens\n        self.index_to_word = index_to_word\n        self.print_every = print_every\n        self.k = top_k\n\n    def sample_from(self, logits):\n        logits, indices = ops.top_k(logits, k=self.k, sorted=True)\n        indices = np.asarray(indices).astype(\"int32\")\n        preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n        preds = np.asarray(preds).astype(\"float32\")\n        return np.random.choice(indices, p=preds)\n\n    def detokenize(self, number):\n        return self.index_to_word[number]\n\n    def on_epoch_end(self, epoch, logs=None):\n        start_tokens = [_ for _ in self.start_tokens]\n        if (epoch + 1) % self.print_every != 0:\n            return\n        num_tokens_generated = 0\n        tokens_generated = []\n        while num_tokens_generated <= self.max_tokens:\n            pad_len = maxlen - len(start_tokens)\n            sample_index = len(start_tokens) - 1\n            if pad_len < 0:\n                x = start_tokens[:maxlen]\n                sample_index = maxlen - 1\n            elif pad_len > 0:\n                x = start_tokens + [0] * pad_len\n            else:\n                x = start_tokens\n            x = np.array([x])\n            y, _ = self.model.predict(x, verbose=0)\n            sample_token = self.sample_from(y[0][sample_index])\n            tokens_generated.append(sample_token)\n            start_tokens.append(sample_token)\n            num_tokens_generated = len(tokens_generated)\n        txt = \" \".join(\n            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n        )\n        print(f\"generated text:\\n{txt}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:40.720651Z","iopub.execute_input":"2024-03-27T09:34:40.721211Z","iopub.status.idle":"2024-03-27T09:34:40.739790Z","shell.execute_reply.started":"2024-03-27T09:34:40.721167Z","shell.execute_reply":"2024-03-27T09:34:40.738560Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"vocab_size = 20000  # Only consider the top 20k words\nmaxlen = 100  # Max sequence size\nembed_dim = 256  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nfeed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\nbatch_size = 128","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:40.741482Z","iopub.execute_input":"2024-03-27T09:34:40.741879Z","iopub.status.idle":"2024-03-27T09:34:40.757370Z","shell.execute_reply.started":"2024-03-27T09:34:40.741847Z","shell.execute_reply":"2024-03-27T09:34:40.755884Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Shuffle the DataFrame\nhuman_propaganda = human_propaganda.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:40.759691Z","iopub.execute_input":"2024-03-27T09:34:40.760138Z","iopub.status.idle":"2024-03-27T09:34:40.772423Z","shell.execute_reply.started":"2024-03-27T09:34:40.760103Z","shell.execute_reply":"2024-03-27T09:34:40.770750Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Create a vectorization layer\nvectorize_layer = TextVectorization(\n    standardize=custom_standardization,\n    max_tokens=vocab_size - 1,\n    output_mode=\"int\",\n    output_sequence_length=maxlen + 1,\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:40.774061Z","iopub.execute_input":"2024-03-27T09:34:40.774503Z","iopub.status.idle":"2024-03-27T09:34:40.825176Z","shell.execute_reply.started":"2024-03-27T09:34:40.774468Z","shell.execute_reply":"2024-03-27T09:34:40.823854Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Adapt the vectorization layer to your text data\nvectorize_layer.adapt(human_propaganda['text'].values)\nvocab = vectorize_layer.get_vocabulary()","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:40.827100Z","iopub.execute_input":"2024-03-27T09:34:40.828404Z","iopub.status.idle":"2024-03-27T09:34:41.151103Z","shell.execute_reply.started":"2024-03-27T09:34:40.828363Z","shell.execute_reply":"2024-03-27T09:34:41.149431Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Create a TensorFlow dataset from DataFrame\ntext_ds = tf.data.Dataset.from_tensor_slices(human_propaganda['text'].values)\n\n# Shuffle and batch the dataset\ntext_ds = text_ds.shuffle(buffer_size=len(human_propaganda)).batch(batch_size)\n\n# Map the prepare_lm_inputs_labels function to the dataset\ntext_ds = text_ds.map(prepare_lm_inputs_labels, num_parallel_calls=tf.data.AUTOTUNE)\n\n# Prefetch the dataset\ntext_ds = text_ds.prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:41.153619Z","iopub.execute_input":"2024-03-27T09:34:41.154042Z","iopub.status.idle":"2024-03-27T09:34:41.353752Z","shell.execute_reply.started":"2024-03-27T09:34:41.154009Z","shell.execute_reply":"2024-03-27T09:34:41.352203Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"word_to_index = {}\nfor index, word in enumerate(vocab):\n    word_to_index[word] = index\n\nstart_prompt = \"In recent months\"\nstart_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\nnum_tokens_generated = 40\ntext_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:41.358144Z","iopub.execute_input":"2024-03-27T09:34:41.358638Z","iopub.status.idle":"2024-03-27T09:34:41.382209Z","shell.execute_reply.started":"2024-03-27T09:34:41.358599Z","shell.execute_reply":"2024-03-27T09:34:41.380856Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model = create_model()\n\nmodel.fit(text_ds, verbose=2, epochs=25, callbacks=[text_gen_callback])","metadata":{"execution":{"iopub.status.busy":"2024-03-27T09:34:41.383824Z","iopub.execute_input":"2024-03-27T09:34:41.384358Z","iopub.status.idle":"2024-03-27T13:07:54.974152Z","shell.execute_reply.started":"2024-03-27T09:34:41.384297Z","shell.execute_reply":"2024-03-27T13:07:54.970472Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch 1/25\ngenerated text:\n[UNK] recent months https : / /t .co /8u9sqguyrv                                   \n\n51/51 - 465s - 9s/step - loss: 2.9838\nEpoch 2/25\ngenerated text:\n[UNK] recent months to its russian gas , a briefing for the us in the russian federation and ukraine , a meeting , which are not the us                \n\n51/51 - 494s - 10s/step - loss: 1.2019\nEpoch 3/25\ngenerated text:\n[UNK] recent months from the us and a [UNK] in the west bank of [UNK] , but it was held talks with president vladimir putin .‚Ä¶                  \n\n51/51 - 519s - 10s/step - loss: 1.0388\nEpoch 4/25\ngenerated text:\n[UNK] recent months and ukraine crisis https : / /t .co [UNK]                                \n\n51/51 - 509s - 10s/step - loss: 0.9264\nEpoch 5/25\ngenerated text:\n[UNK] recent months of russia 's foreign ministers council of [UNK] ) https : / /t .co [UNK] https : / /t .co / /t .co                  \n\n51/51 - 479s - 9s/step - loss: 0.8395\nEpoch 6/25\ngenerated text:\n[UNK] recent months of foreign ministry https : / /t .co [UNK]                                \n\n51/51 - 507s - 10s/step - loss: 0.7652\nEpoch 7/25\ngenerated text:\n[UNK] recent months of the ukrainian neo -nazis - reuters https : / /t .co /8u9sqguyrv https : / /t .co #russianembassy https : / /t .co ne‚Ä¶                \n\n51/51 - 571s - 11s/step - loss: 0.6989\nEpoch 8/25\ngenerated text:\n[UNK] recent months , but it is now it to be free speech right to join us , and t‚Ä¶ https : / /t .co /3xox7ecvxx                  \n\n51/51 - 513s - 10s/step - loss: 0.6395\nEpoch 9/25\ngenerated text:\n[UNK] recent months of the russian disinformation or brother https : / /t .co /tbvlesi6pm https : / /t .co /cnlber3dpe                       \n\n51/51 - 510s - 10s/step - loss: 0.5859\nEpoch 10/25\ngenerated text:\n[UNK] recent months of war crime ? https : / /t .co /azwnubm9v4 https : / /t .co /efbsoew5n3                         \n\n51/51 - 473s - 9s/step - loss: 0.5374\nEpoch 11/25\ngenerated text:\n[UNK] recent months of russia is a number of ukraine . [UNK] ) ‚û° https : / /t .co [UNK] https : / /t .co [UNK]                  \n\n51/51 - 539s - 11s/step - loss: 0.4921\nEpoch 12/25\ngenerated text:\n[UNK] recent months , kiev is set to have reportedly came from the west to evacuate and after the area . over w‚Ä¶ https : / /t .co /juxjrtcmp3               \n\n51/51 - 506s - 10s/step - loss: 0.4476\nEpoch 13/25\ngenerated text:\n[UNK] recent months of russian oil , gas exports grew by police cars , with at the develo‚Ä¶ https : / /t .co /z7bvafypgy                    \n\n51/51 - 569s - 11s/step - loss: 0.4033\nEpoch 14/25\ngenerated text:\n[UNK] recent months of russian diplomats who was officially resettled in https : / /t .co /dj5f3fs1kf                           \n\n51/51 - 471s - 9s/step - loss: 0.3569\nEpoch 15/25\ngenerated text:\n[UNK] recent months after week , the first will be made by @jy _ledrian , who accused of peace or was‚Ä¶ https : / /t .co /byntpta5uo                 \n\n51/51 - 511s - 10s/step - loss: 0.3121\nEpoch 16/25\ngenerated text:\n[UNK] recent months , will hit the eu nations has grown 15 after it was unveiled https : / /t .co /gut5gjwdxo                      \n\n51/51 - 507s - 10s/step - loss: 0.2701\nEpoch 17/25\ngenerated text:\n[UNK] recent months of kiev will hold another nation ' : ‚ùóÔ∏èthe united states is not true to exclude russia , ' on its allies , the‚Ä¶                 \n\n51/51 - 560s - 11s/step - loss: 0.2325\nEpoch 18/25\ngenerated text:\n[UNK] recent months as bulgaria from the gas austria‚Äôs omv is not to make it is seeking to make t‚Ä¶ https : / /t .co /zqje4m0ybg                  \n\n51/51 - 513s - 10s/step - loss: 0.1994\nEpoch 19/25\ngenerated text:\n[UNK] recent months as is another act or dissent ? as populism rises in the west , so do crackdowns on narratives t‚Ä¶ https : / /t .co /owj9u9gzau               \n\n51/51 - 474s - 9s/step - loss: 0.1722\nEpoch 20/25\ngenerated text:\n[UNK] recent months as pretext for first time ? https : / /t .co /bihstjdw80                             \n\n51/51 - 507s - 10s/step - loss: 0.1513\nEpoch 21/25\ngenerated text:\n[UNK] recent months of the russian and the black sea fleet 's has successfully completed its crew on their first official languages of the ukrainian‚Ä¶                   \n\n51/51 - 505s - 10s/step - loss: 0.1343\nEpoch 22/25\ngenerated text:\n[UNK] recent months as biker rally hits canada‚Äôs capital the capital was warned that bloody buffalo , is‚Ä¶ https : / /t .co /unu3fzdjr2                    \n\n51/51 - 513s - 10s/step - loss: 0.1217\nEpoch 23/25\ngenerated text:\n[UNK] recent months of the most and ukraine ' - reuters news conference on spam and fake news [UNK] fake [UNK] c‚Ä¶ https : / /t .co /unbcpo566m                \n\n51/51 - 469s - 9s/step - loss: 0.1117\nEpoch 24/25\ngenerated text:\n[UNK] recent months of the russian coal and the black sea fleet launched a four years pretending to ukraine intelligence ve‚Ä¶ https : / /t .co /sstwj7t3pt                 \n\n51/51 - 542s - 11s/step - loss: 0.1033\nEpoch 25/25\ngenerated text:\n[UNK] recent months , not received ukraine from peace &amp ; gas exports to the dod does not impossible ‚Äî local un‚Ä¶ https : / /t .co /xqa6uwkflm                \n\n51/51 - 565s - 11s/step - loss: 0.0967\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7a946d3d8190>"},"metadata":{}}]}]}