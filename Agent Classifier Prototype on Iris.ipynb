{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "28p9bfvdSGHu"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGfKr6QHIrcj",
        "outputId": "8fc0491f-1b79-4151-ff7b-dc9b3ba2d764"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.8/250.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --quiet mesa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import joblib\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "joblib.dump(clf, 'iris_classifier.joblib')\n",
        "\n",
        "loaded_model = joblib.load('iris_classifier.joblib')\n",
        "\n",
        "# For example, predict the first two samples from the test set\n",
        "predictions = loaded_model.predict(X_test[:2])\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg_GL-S_UEm5",
        "outputId": "8c3dff57-1a81-483d-fc6a-8694158ab3aa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Iris Dataset\n",
        "iris = load_iris()\n",
        "class_names = iris.target_names\n",
        "\n",
        "# Split Data into Train and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and Train the Classifier\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Save the Model to Disk\n",
        "joblib.dump(clf, 'iris_classifier.joblib')\n",
        "\n",
        "# Load the Model from Disk\n",
        "loaded_model = joblib.load('iris_classifier.joblib')\n",
        "\n",
        "# Make Predictions on Sample Data\n",
        "# For example, predict the first two samples from the test set\n",
        "predictions = loaded_model.predict(X_test[:2])\n",
        "\n",
        "# Print Predictions with String Names\n",
        "for prediction in predictions:\n",
        "    print(class_names[prediction])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xFR-7VIUtov",
        "outputId": "0f099118-ea2f-44ed-e527-b8bac02a146a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "versicolor\n",
            "setosa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxLmG7pcUyUo",
        "outputId": "da8bb277-6278-4b25-d28c-099abc4ef8b5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.1, 2.8, 4.7, 1.2],\n",
              "       [5.7, 3.8, 1.7, 0.3]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mesa import Agent, Model\n",
        "from mesa.time import RandomActivation\n",
        "import random\n",
        "import joblib\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class Informer(Agent):\n",
        "    def __init__(self, unique_id, model, classifier_agent):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.iris = load_iris()\n",
        "        self.X_test = self.split_data()\n",
        "        self.classifier_agent = classifier_agent\n",
        "\n",
        "    def split_data(self):\n",
        "        X = self.iris.data\n",
        "        y = self.iris.target\n",
        "        _, X_test, _, _ = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "        return X_test\n",
        "\n",
        "    def step(self):\n",
        "        random_sample = random.choice(self.X_test)\n",
        "        self.classifier_agent.predict_sample(random_sample)\n",
        "\n",
        "class Classifier(Agent):\n",
        "    def __init__(self, unique_id, model, evaluator_agent):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.loaded_model = joblib.load('iris_classifier.joblib')\n",
        "        self.evaluator_agent = evaluator_agent\n",
        "\n",
        "    def predict_sample(self, sample):\n",
        "        prediction = self.loaded_model.predict([sample])[0]\n",
        "        self.evaluator_agent.evaluate(prediction)\n",
        "\n",
        "class Evaluator(Agent):\n",
        "    def __init__(self, unique_id, model):\n",
        "        super().__init__(unique_id, model)\n",
        "\n",
        "    def evaluate(self, prediction):\n",
        "        if prediction == 0:\n",
        "            print(\"Iris setosa\")\n",
        "        elif prediction == 1:\n",
        "            print(\"Iris versicolor\")\n",
        "        else:\n",
        "            print(\"Iris virginica\")\n",
        "\n",
        "class IrisModel(Model):\n",
        "    def __init__(self, num_informers, num_classifiers, num_evaluators):\n",
        "        self.num_informers = num_informers\n",
        "        self.num_classifiers = num_classifiers\n",
        "        self.num_evaluators = num_evaluators\n",
        "        self.schedule = RandomActivation(self)\n",
        "\n",
        "        # Create agents\n",
        "        evaluator_agent = Evaluator(0, self)\n",
        "        self.schedule.add(evaluator_agent)\n",
        "\n",
        "        classifier_agent = Classifier(1, self, evaluator_agent)\n",
        "        self.schedule.add(classifier_agent)\n",
        "\n",
        "        for i in range(self.num_informers):\n",
        "            informer = Informer(i, self, classifier_agent)\n",
        "            self.schedule.add(informer)\n",
        "\n",
        "    def step(self):\n",
        "        self.schedule.step()\n",
        "\n",
        "# Create and run the model\n",
        "model = IrisModel(num_informers=1, num_classifiers=1, num_evaluators=1)\n",
        "for i in range(10):  # Run for 10 steps\n",
        "    model.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz_JokgdVnd9",
        "outputId": "e5f7f121-5719-41c1-cf33-35a72ded7ae6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-17db1e759acd>:37: FutureWarning: The Mesa Model class was not initialized. In the future, you need to explicitly initialize the Model by calling super().__init__() on initialization.\n",
            "  super().__init__(unique_id, model)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris virginica\n",
            "Iris versicolor\n",
            "Iris setosa\n",
            "Iris versicolor\n",
            "Iris versicolor\n",
            "Iris versicolor\n",
            "Iris virginica\n",
            "Iris virginica\n",
            "Iris versicolor\n",
            "Iris setosa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **DRAFT**"
      ],
      "metadata": {
        "id": "28p9bfvdSGHu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Informer anf Data Preprocessor**"
      ],
      "metadata": {
        "id": "M5RWU-R7IuoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mesa import Agent, Model\n",
        "from mesa.time import RandomActivation"
      ],
      "metadata": {
        "id": "Qg0-6tWfI0mx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Informer_Agent(Agent):\n",
        "    def __init__(self, unique_id, model, message):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.message = message\n",
        "\n",
        "    def step(self):\n",
        "        # Submit the message to the data preprocessor agent\n",
        "        self.model.data_preprocessor.receive_message(self.message)"
      ],
      "metadata": {
        "id": "OGsnFchtI2sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreprocessorAgent(Agent):\n",
        "    def __init__(self, unique_id, model):\n",
        "        super().__init__(unique_id, model)\n",
        "\n",
        "    def receive_message(self, message):\n",
        "        # Here you can process the received message\n",
        "        print(\"Received message:\", message)"
      ],
      "metadata": {
        "id": "DwVxLkC7KcFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras import layers, Input, Model, ops\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "data = pd.read_csv('/content/twitter_dataset.csv')\n",
        "df = data[['text', 'is_propaganda']]\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "\n",
        "@keras.utils.register_keras_serializable(package='Custom', name='TransformerBlock')\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, trainable=True, rate=0.1, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.trainable = trainable\n",
        "\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'num_heads': self.num_heads,\n",
        "            'ff_dim': self.ff_dim,\n",
        "            'trainable': self.trainable,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "@keras.utils.register_keras_serializable(package='Custom', name='TokenAndPositionEmbedding')\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim, trainable=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.maxlen = maxlen\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.trainable = trainable\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.token_emb = layers.Embedding(input_dim=self.vocab_size, output_dim=self.embed_dim, trainable=self.trainable)\n",
        "        self.pos_emb = layers.Embedding(input_dim=self.maxlen, output_dim=self.embed_dim, trainable=self.trainable)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=self.maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'maxlen': self.maxlen,\n",
        "            'vocab_size': self.vocab_size,\n",
        "            'embed_dim': self.embed_dim,\n",
        "            'trainable': self.trainable\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        return cls(**config)\n",
        "\n",
        "\n",
        "\n",
        "vocab_size = 20000  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review\n",
        "\n",
        "features = df['text']\n",
        "labels = df['is_propaganda']\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(features)\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences(features)\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "padded_sequences = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.5, random_state=42)\n",
        "\n",
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = Input(shape=(maxlen,))\n",
        "#embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "# Instantiate TokenAndPositionEmbedding with trainable argument\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim, trainable=False)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(60, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(40, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(20, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    x_train, y_train, batch_size=32, epochs=20, validation_data=(x_val, y_val)\n",
        ")\n",
        "\n",
        "model.save(\"propaganda_classifier.keras\")"
      ],
      "metadata": {
        "id": "zch36T-zKf5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!ls -lha kaggle.json\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle #Create the directory\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d bohdanmynzar/twitter-propaganda-classification\n",
        "!unzip twitter-propaganda-classification.zip\n",
        "!pip install --upgrade keras"
      ],
      "metadata": {
        "id": "rT6lNRqVLfyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# Sample data\n",
        "texts = [\"I love this product!\", \"This is terrible.\", \"It's okay, not great.\"]\n",
        "\n",
        "# Corresponding labels\n",
        "labels = [\"positive\", \"negative\", \"negative\"]\n",
        "\n",
        "# Preprocess data\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts)\n",
        "\n",
        "# Train classifier\n",
        "classifier = make_pipeline(CountVectorizer(), SVC(kernel='linear'))\n",
        "classifier.fit(texts, labels)\n",
        "\n",
        "# Test the classifier\n",
        "test_texts = [\"I'm happy with this purchase.\", \"Worst experience ever!\"]\n",
        "predictions = classifier.predict(test_texts)\n",
        "\n",
        "# Output predictions\n",
        "for text, prediction in zip(test_texts, predictions):\n",
        "    print(f\"Text: {text} \\nPredicted Label: {prediction}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hyYvXnLMVKW",
        "outputId": "87a63225-bab5-457a-cf61-9e1426928a2b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I'm happy with this purchase. \n",
            "Predicted Label: negative\n",
            "\n",
            "Text: Worst experience ever! \n",
            "Predicted Label: negative\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from mesa import Agent, Model\n",
        "from mesa.time import BaseScheduler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "class InformerAgent(Agent):\n",
        "    def __init__(self, unique_id, model, initial_string):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.initial_string = initial_string\n",
        "\n",
        "    def step(self):\n",
        "        # Send the initial string to the preprocessor agent\n",
        "        self.model.preprocessor_agent.receive_string(self.initial_string)\n",
        "\n",
        "class PreprocessorAgent(Agent):\n",
        "    def __init__(self, unique_id, model):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.vectorizer = CountVectorizer()\n",
        "\n",
        "    def preprocess_string(self, string):\n",
        "        # Preprocess the string\n",
        "        preprocessed_string = self.vectorizer.transform([string])\n",
        "        return preprocessed_string\n",
        "\n",
        "    def receive_string(self, string):\n",
        "        preprocessed_string = self.preprocess_string(string)\n",
        "        # Send the preprocessed string to the classifier agent\n",
        "        self.model.classifier_agent.receive_preprocessed_string(preprocessed_string)\n",
        "\n",
        "class ClassifierAgent(Agent):\n",
        "    def __init__(self, unique_id, model):\n",
        "        super().__init__(unique_id, model)\n",
        "        self.classifier = make_pipeline(CountVectorizer(), SVC(kernel='linear'))\n",
        "\n",
        "    def classify_string(self, preprocessed_string):\n",
        "        # Classify the preprocessed string\n",
        "        prediction = self.classifier.predict(preprocessed_string)[0]\n",
        "        return prediction\n",
        "\n",
        "    def receive_preprocessed_string(self, preprocessed_string):\n",
        "        prediction = self.classify_string(preprocessed_string)\n",
        "        # Send the prediction to the evaluator agent\n",
        "        self.model.evaluator_agent.receive_prediction(prediction)\n",
        "\n",
        "class EvaluatorAgent(Agent):\n",
        "    def __init__(self, unique_id, model):\n",
        "        super().__init__(unique_id, model)\n",
        "\n",
        "    def evaluate_prediction(self, prediction):\n",
        "        # Evaluate the prediction and display a message\n",
        "        if prediction == \"positive\":\n",
        "            print(\"The text is classified as positive.\")\n",
        "        elif prediction == \"negative\":\n",
        "            print(\"The text is classified as negative.\")\n",
        "        else:\n",
        "            print(\"The text cannot be classified.\")\n",
        "\n",
        "    def receive_prediction(self, prediction):\n",
        "        self.evaluate_prediction(prediction)\n",
        "\n",
        "class TextClassificationModel(Model):\n",
        "    def __init__(self, initial_string):\n",
        "        self.initial_string = initial_string\n",
        "        self.scheduler = BaseScheduler(self)\n",
        "\n",
        "        # Initialize agents\n",
        "        self.informer_agent = InformerAgent(1, self, initial_string)\n",
        "        self.preprocessor_agent = PreprocessorAgent(2, self)\n",
        "        self.classifier_agent = ClassifierAgent(3, self)\n",
        "        self.evaluator_agent = EvaluatorAgent(4, self)\n",
        "\n",
        "        # Add agents to the scheduler\n",
        "        self.scheduler.add(self.informer_agent)\n",
        "        self.scheduler.add(self.preprocessor_agent)\n",
        "        self.scheduler.add(self.classifier_agent)\n",
        "        self.scheduler.add(self.evaluator_agent)\n",
        "\n",
        "    def step(self):\n",
        "        self.scheduler.step()\n",
        "\n",
        "# Create and run the model\n",
        "initial_string = \"This product is amazing!\"\n",
        "model = TextClassificationModel(initial_string)\n",
        "model.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4Edx4w6RQMWK",
        "outputId": "e119b44c-293a-4d00-d2dd-2f89fcbe1e25"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-860532a3cdf6>:9: FutureWarning: The Mesa Model class was not initialized. In the future, you need to explicitly initialize the Model by calling super().__init__() on initialization.\n",
            "  super().__init__(unique_id, model)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "Vocabulary not fitted or provided",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-860532a3cdf6>\u001b[0m in \u001b[0;36m<cell line: 85>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0minitial_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"This product is amazing!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextClassificationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-860532a3cdf6>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Create and run the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mesa/time.py\u001b[0m in \u001b[0;36m_wrapped_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wrapped_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;34m\"\"\"Wrapper for the step method to include time and step updating.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_advance_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mesa/time.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# To be able to remove and/or add agents during stepping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;31m# it's necessary for the keys view to be a list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_each\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mesa/time.py\u001b[0m in \u001b[0;36mdo_each\u001b[0;34m(self, method, shuffle)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mesa/agent.py\u001b[0m in \u001b[0;36mdo\u001b[0;34m(self, method_name, return_results, *args, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m         \"\"\"\n\u001b[1;32m    244\u001b[0m         \u001b[0;31m# we iterate over the actual weakref keys and check if weakref is alive before calling the method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         res = [\n\u001b[0m\u001b[1;32m    246\u001b[0m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0magentref\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyrefs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/mesa/agent.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;31m# we iterate over the actual weakref keys and check if weakref is alive before calling the method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         res = [\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0magentref\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyrefs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0magentref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-860532a3cdf6>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Send the initial string to the preprocessor agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessor_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mPreprocessorAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-860532a3cdf6>\u001b[0m in \u001b[0;36mreceive_string\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreceive_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mpreprocessed_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Send the preprocessed string to the classifier agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_preprocessed_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-860532a3cdf6>\u001b[0m in \u001b[0;36mpreprocess_string\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Preprocess the string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mpreprocessed_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpreprocessed_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                 \u001b[0;34m\"Iterable over raw text documents expected, string object received.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m             )\n\u001b[0;32m-> 1430\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
          ]
        }
      ]
    }
  ]
}